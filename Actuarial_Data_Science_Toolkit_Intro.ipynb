{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc3LVh8qXBCK"
      },
      "source": [
        "> [Welcome to Colab](https://colab.research.google.com/), *a hosted `python` notebook environment, where code & documentation live side-by-side.*\n",
        ">\n",
        "> **Note:** if your notebook sections are expanded, please go to *`View > Collapse sections`* or press `Ctrl/âŒ˜ + [`\n",
        "---\n",
        "\n",
        "---\n",
        "[<img src=\"https://upload.wikimedia.org/wikipedia/de/6/62/International_Actuarial_Association_Logo.svg\" width=\"100\" height=\"100\">](https://www.actuaries.org/iaa)\n",
        "\n",
        "[<img src=\"https://www.actuarialsociety.org.za/wp-content/uploads/2023/03/75-ASSA-Logo.png\" width=\"500\" >](https://www.actuarialsociety.org.za/)\n",
        "\n",
        "# **The Actuarial Data Science Toolkit** \n",
        "### *A Practical Introduction*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SryZKhcAQKve"
      },
      "source": [
        "# **0. Table of contents** \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVIrkzpfQVgR"
      },
      "source": [
        "â†–ï¸ `click  task bar`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNuDdEzFL0Td"
      },
      "source": [
        "# **1. Introduction**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7A1rKE8QhNP"
      },
      "source": [
        "\n",
        "In recent years the rise of data intensive methods has seen terms like *Big Data*, *Data Science* and *Arficial Intelligence* (AI) enter public lexicon. The use of such methods in data-rich, mostly internet-based businesses has resulted in tools that may be applied more broadly and indeed data science tools and methods are used in broad domains. Actuaries have deep expertise with data, yet we have been slow to adapt to use newer data science type tools. \n",
        "\n",
        ">   â„¹ï¸ *An actuarial view of Data Science may be found [here](https://actuaries.org.uk/learn/lifelong-learning/data-science-an-actuarial-viewpoint/) and an overview of AI in Actuarial Science may be found [here](https://www.actuarialsociety.org.za/wp-content/uploads/2018/10/2018-Richman-FIN.pdf).*\n",
        "\n",
        "This practical tutorial is designed to *introduce* data science tools that may prove useful in actuarial workflows. No prior knowledge is assumed and concepts are outlined with provided **reusable code**. Links to more thorough learning materials for individual topics are also provided. The hope is that it will spark curiousity in members and that they will use such tools more regularly in their day-to-day work. \n",
        "\n",
        ">   â„¹ï¸ *Typical actuarial work needs better data maturity, along with practical upskilling to take full advantage of advanced AI. As a profession of thought leaders we should help shape our industries' futures to take advantage of these rapidly evolving technologies.*\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtYRhAkvQSXv"
      },
      "source": [
        "# **2. From Excel to `python`**  ðŸ\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7aj5BRfZg5P"
      },
      "source": [
        "\n",
        "#### **2.0 Section learning objectives**\n",
        "\n",
        ">The purpose of this section is to introduce a member, who is proficient in Excel, to basic `python` tools. This is intended for an absolute beginner and  afterwards the member should feel comfortable using `python` for common tasks they would normally perform in Excel.\n",
        ">\n",
        ">   * Basic system operation\n",
        ">       * Introduce notebooks\n",
        ">       * `wget` data \n",
        ">       * `os` functionality\n",
        ">       * `pip install`\n",
        ">       * `import` packages\n",
        ">\n",
        ">   * `pandas`\n",
        ">       * Link to learning resoruces\n",
        ">       * Note on `v2.0.0` \n",
        ">       * Summarise data\n",
        ">       * Read `.xlsx` data into df\n",
        ">       * Rename columns\n",
        ">       * Drop `na`\n",
        ">       * Query with wildcards\n",
        ">       * Filter \n",
        ">       * Conditional formatting\n",
        ">\n",
        ">   * Mention `openpyxl` & `xlwings`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3KFkU08M-Vu"
      },
      "source": [
        "#### 2.1 **We can't ignore Excel, for now...**\n",
        "\n",
        "There is no denying that the majority of actuarial work is done in Excel (for now!). As such, learning to intergrate Excel as a tool in your data science workflow is essential for practical benefit. \n",
        "\n",
        "\n",
        "#### 2.2 **Why `python`?**\n",
        "\n",
        "In the data science community there has been fierce debate over whether `python` or `R` was the better data science coding language. While both languages are capible, we have opted for `python` as it has become the defacto language used for modern AI tools. It is also worth mentioning that `julia`, a newer language is growing and there is already an [actuarial `julia` community](https://juliaactuary.org/).\n",
        "\n",
        "A major advantage of Python is the popularity of notebooks, like this document. A notebook is a \"living document\" with live code and documentation displayed side-by-side. We have opted to use Colab as it provides free GPU access, needed for modern AI. We will need this GPU below!  \n",
        "\n",
        "#### 2.3 **Getting data from an external source**\n",
        "GitHub is a platform used to store code. In making this tutorial we created the Actuarial Society's GitHUb account. \n",
        "\n",
        "> â„¹ï¸ *GitHub is a where open-source code is stored. It is a great place to colaborate and there are lots of free-to-use data science projects (repositories) to explore. If you don't have an account you can get one [here](https://github.com/).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YI6knoNo-0lv"
      },
      "outputs": [],
      "source": [
        "# clone the repository from the actuarialsociety's github \n",
        "# bash script (!)\n",
        "!git clone https://github.com/actuarialsociety/dstoolkit.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUop0CTHpwqj"
      },
      "source": [
        "Let's check that our files are there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYWe2EKXp7q0"
      },
      "outputs": [],
      "source": [
        "# Import os package to enable us to work with files\n",
        "import os\n",
        "print(f\"Getting our current working directory{os.getcwd()} \\n\")\n",
        "print(f\"Listing what is in our directory \\n {os.listdir()} \\n\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G48kL10KPN2"
      },
      "source": [
        "We can see the repository we just cloned `dstoolkit` is listed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXIiRI2gKOHr"
      },
      "outputs": [],
      "source": [
        "# Examining our repo\n",
        "os.listdir(\"./dstoolkit\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA353EXPOZuY"
      },
      "source": [
        " â†–ï¸ We could have also just used the task bar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueR7pJHdEZh0"
      },
      "source": [
        "#### 2.4 **How to use `python` to work with Excel**\n",
        "\n",
        "In order to extend the functionality of basic `python` we need to import **packages** into our notebook environment, into which all code is executed. Packages allow for an extension of basic python functionality.\n",
        "\n",
        "The `python` pacakges `numpy` and `pandas` are literally two of the most used packages across any computing language. `numpy` provides broad mathematical functionality and `pandas` allows for data manipulation from multiple sources, including spreadsheets like Excel. \n",
        "In order to extend the functionality of basic `python` we need to import packages into our notebook environment, into which all code is executed. The `python` pacakges `numpy` and `pandas` are literally two of the most used packages across any computing language. `numpy` provides broad mathematical functionality and `pandas` allows for data manipulation from multiple sources, including spreadsheets like Excel. \n",
        "\n",
        "Typically packages are installed into our `python` environment using the `pip` command, however as we see below, due to their popularity, these packages are already installed in the Colab notebook environment. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwHUlMhRV6GG"
      },
      "outputs": [],
      "source": [
        "# bash script (!)\n",
        "# we use -q (quiet) to suppress the output\n",
        "%pip install numpy pandas Jinja2 sklearn openpyxl matplotlib seaborn scikit-learn -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzuLZRulv59l"
      },
      "source": [
        "> â„¹ï¸ `pandas` has very good documentation and a short introduction can be found [here](https://pandas.pydata.org/docs/user_guide/10min.html).\n",
        "\n",
        "`pandas` a very  capable I/O API (Input/Output capabilities) and can read data from a [variety of sources](https://pandas.pydata.org/docs/user_guide/io.html), including common cloud platforms.\n",
        "\n",
        "> â„¹ï¸ *`pandas` version 2 (`pandas == 2.0`) was recently released. You can find the release notes [here](https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html#), as well as a short video on what is new [here](https://www.youtube.com/watch?v=cSLPyRI_ZD8).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVTqVAkqR_xC"
      },
      "outputs": [],
      "source": [
        "# import necessary packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# path to data we just downloaded\n",
        "# if you are interested in how modelpoints were created,\n",
        "# check out the `generating_modelpoints.ipynb` file in the repo\n",
        "DATA_XLS = \"./dstoolkit/model_point.xlsx\"\n",
        "\n",
        "# ingesting our data into a dataframe (df)\n",
        "df = pd.read_excel(DATA_XLS)\n",
        "\n",
        "\n",
        "# now printing out some information about our data\n",
        "print(f\"\\n Getting some basic info about our data... \\n\")\n",
        "print(f\"* size: {df.size} \\n\")\n",
        "print(f\"* shape:{df.shape} \\n\")\n",
        "\n",
        "# now disaplying some information\n",
        "# `display` is like `print` but renders pd df's better \n",
        "display(df.info())\n",
        "\n",
        "print(\"\\n Getting an idea about the ranges of each column... \\n\")\n",
        "display(df.describe())\n",
        "\n",
        "display(df.head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsuk9RS5Rd9U"
      },
      "source": [
        "We can already see from our `model_point.info()` command that there are `null` values, that we probably want to drop. In reality it probably worth checking if the data could be better sourced to have fewer `null` values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3nPSgqZUpsX"
      },
      "outputs": [],
      "source": [
        "print(f\"\\n We examine the number of null values in our data before dropping them... \\n\")\n",
        "print(f\"\\n Number of null items: \\n {df.isnull( ).sum()}\")\n",
        "print(f\"\\n Shape before dropping nulls: \\n {df.shape}\")\n",
        "df = df.dropna()\n",
        "print(f\"\\n Shape after dropping nulls: \\n {df.shape}\")\n",
        "\n",
        "print(f\"\\n We probably also what to trim spaces in our column names,convert to lower case and replace spaces with _ to make it easier to work with\\n\")\n",
        "new_columns = [str(col).strip().lower().replace(\" \",\"_\") for col in list(df.columns)]\n",
        "print(new_columns)\n",
        "\n",
        "# creating dict to rename df\n",
        "rename_dict = dict()\n",
        "for i in range(0,len(list(df.columns))):\n",
        "  rename_dict.update({list(df.columns)[i]:new_columns[i]})\n",
        "\n",
        "# renaming df\n",
        "df = df.rename(columns=rename_dict)\n",
        "\n",
        "# coverting df.sex to a Categorical variable\n",
        "df.sex = pd.Categorical(df.sex)\n",
        "\n",
        "# coverting df.issue to a datetime\n",
        "df.issue_date =  pd.to_datetime(df.issue_date, format='%d/%m/%Y')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBicWNv83ffq"
      },
      "outputs": [],
      "source": [
        "# Importantly this allows one to access columns easier when running queries\n",
        "# Setting query wildcards\n",
        "query_age = 55\n",
        "query_sa = 100000\n",
        "\n",
        "# Querying with wildcards\n",
        "(df\n",
        " .query('age_at_entry > @query_age and sum_assured > @query_sa')\\\n",
        " .head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJU_h_4YAZHm"
      },
      "outputs": [],
      "source": [
        "# We may wish to group by certrain columns and perform aggregations\n",
        "(df\n",
        ".groupby(['sex','policy_term'])\\\n",
        ".sum_assured\\\n",
        ".agg(['sum', 'mean','count']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdD8RQm-CZMz"
      },
      "outputs": [],
      "source": [
        "# We may also wish to perform conditional formatting on our dataframe\n",
        "\n",
        "(df.\n",
        " sort_values('age_at_entry').head(5)[['age_at_entry', 'premium']]\\\n",
        ".style\\\n",
        ".background_gradient(\"Blues\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pulling in an **external source of underwriter outcomes**, we want to apply discounts to applicable policies in our modelpoint. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "UW_CSV = pd.read_csv('underwriter_outcomes.csv', index_col=0)\n",
        "\n",
        "df_uw = pd.read_csv(UW_CSV, index_col=0)\n",
        "\n",
        "display(df_uw.head(3))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Making sure our modelpoint's IDs are actually in those that extract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"{len(df.uuid.isin(df_uw.uuid))} out of {len(df.uuid)} are in the underwriter outcomes file\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Making sure the types of our columns match\n",
        "df_uw['uuid'] = df_uw['uuid'].astype(str)\n",
        "df['uuid'] = df['uuid'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# left joining model_point_table and underwriter_outcomes\n",
        "\n",
        "DISC = 0.1\n",
        "df_disc = df.merge(df_uw, on='uuid', how='left')\n",
        "\n",
        "df_disc = final_premium = np.where(df_disc.discount==True,\n",
        "                                                   df_disc.premium * (1 - DISC),df.premium)\n",
        "\n",
        "df_disc"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KCSX4thlCQGF"
      },
      "source": [
        "> * â„¹ï¸ *To read named ranges from Excel one can use [`openpyxl`](https://openpyxl.readthedocs.io/en/stable/)*\n",
        "> * â„¹ï¸ *Alternatively [`xlwings`](https://www.xlwings.org/) allows one to read named ranges and allows one to replace VBA with python. There are some basic commands explored in `./dstoolkit/Advanced_Excel_with_Python-xlwings.ipynb`*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnnTMc0luTig"
      },
      "source": [
        "# **3. Make it visual ðŸ‘€** \n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vct36LBila4_"
      },
      "source": [
        "#### **3.0 Section learning objectives** \n",
        "\n",
        ">The purpose of this section is to build upon the previous section; having ingested data, simple visualisation techniques are explored\n",
        ">  * Slice list\n",
        ">  * Basic plotting by looping with `matplotlib.pyplot` & `seaborn`\n",
        ">  * Reduce distinct x-axis values for better visualisation\n",
        ">  * Write & use a basic python function for repeated code\n",
        ">  * Docstrings from own function and from packages\n",
        "  \n",
        "After looking at all available columns, we decide we wish to drop `'model_point'` & `'uuid'`, and thus only focus on columns with index 2 and above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwnMYitKfDa4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#only focusing on columns with index 2 and above.\n",
        "cols_to_plot = list(df.columns)[2:]\n",
        "print(cols_to_plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZLKhjuxvut0"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set a Seaborn style=\"white\", \"dark\", \"whitegrid\", \"darkgrid\", \"ticks\"\n",
        "sns.set(style=\"darkgrid\", font_scale=1.1)\n",
        "\n",
        "# Define subplot grid\n",
        "fig, axs = plt.subplots(nrows=int(len(cols_to_plot) / 2) , ncols=2, figsize=(15, 12))\n",
        "\n",
        "# Set figure title\n",
        "fig.suptitle(\"Examining df... take 1\", fontsize=18, y=0.95)\n",
        "\n",
        "# Loop through cols_to_plot\n",
        "for col, ax in zip(cols_to_plot, axs.ravel()):\n",
        "    # Getting an idea of the distribution by counting values\n",
        "    counts = df[col].value_counts().reset_index()\n",
        "    \n",
        "    # Use Seaborn barplot with custom colors\n",
        "    sns.barplot(data=counts, x='index', y=col, ax=ax, palette='viridis')\n",
        "\n",
        "    # Chart formatting\n",
        "    # Chart title\n",
        "    ax.set_title(col)\n",
        "    # Chart y label\n",
        "    ax.set_ylabel(\"count\")\n",
        "    # Chart x label\n",
        "    ax.set_xlabel(\"value\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXHmCR8SQysu"
      },
      "source": [
        "We can see that for `issue_date, sum_assured, premium` there are too many values we are counting by, and thus we struggle to see what is happening. \n",
        "\n",
        "We will create new variables that aggregate these columns for fewer values and replace those columns in our `cols_to_plot` list. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xkzy0vVDS6Kx"
      },
      "outputs": [],
      "source": [
        "df['issue_date_year'] = df['issue_date'].dt.to_period('Y')\n",
        "\n",
        "print(f\"For `issue_date_year`, we previously had {df['issue_date'].nunique() } unique values; now we have {df['issue_date_year'].nunique() } \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnTHYhATg88S"
      },
      "outputs": [],
      "source": [
        "# using list comprehension to create bins\n",
        "bins = [(df['sum_assured'].max())*(i/10) for i in range(1,11)]\n",
        "df['sum_assured_binned'] = pd.cut(df['sum_assured'], bins)\n",
        "\n",
        "print(f\"For `sum_assured`, we previously had {df['sum_assured'].nunique() } unique values; now we have {df['sum_assured_binned'].nunique() } \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkNjyieDkj1-"
      },
      "outputs": [],
      "source": [
        "# using list comprehension to create bins\n",
        "bins = [(df['premium'].max())*(i/10) for i in range(1,11)]\n",
        "df['premium_binned'] = pd.cut(df['premium'], bins)\n",
        "\n",
        "print(f\"For `premium`, we previously had {df['premium'].nunique() } unique values; now we have {df['premium_binned'].nunique() } \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooD957bDmVba"
      },
      "outputs": [],
      "source": [
        "# we make a dictionary to change items in our cols_to_plot list\n",
        "rename_dict = dict()\n",
        "\n",
        "for i in cols_to_plot:\n",
        "  rename_dict.update({i:i})\n",
        "rename_dict[\"issue_date\"] = \"issue_date_year\"\n",
        "rename_dict[\"sum_assured\"] = \"sum_assured_binned\"\n",
        "rename_dict[\"premium\"] = \"premium_binned\"\n",
        "\n",
        "# now we map rename_dict to cols_to_plot \n",
        "cols_to_plot = [*map(rename_dict.get, cols_to_plot)]\n",
        "\n",
        "if 'policy_count' in cols_to_plot: cols_to_plot.remove('policy_count')\n",
        "\n",
        "cols_to_plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imRUnmP8ohZ6"
      },
      "source": [
        "Now that we have updated `cols_to_plot` we will turn our plot commands into a function. Note the description (known as a Doc String) which outlines the detail of inputs and outputs. This will appear if you say `help(function_name)`\n",
        "\n",
        "Notebooks are messy and often used for ad-hoc investigations. In a production environment we would want to turn all our commands into neat functions. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNmb4VfHpIg8"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "def plot_distribution(df: pd.DataFrame, cols_to_plot: List[str], style: str = 'whitegrid', palette: str = 'viridis'):\n",
        "    \"\"\"\n",
        "    Plot the distribution of the specified columns in a DataFrame using Seaborn's barplot.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): The DataFrame containing the columns to plot\n",
        "    cols_to_plot (list): A list of column names to plot\n",
        "    style (str): The Seaborn style to use for the plot. Options: \"darkgrid\", \"whitegrid\", \"dark\", \"white\", \"ticks\"\n",
        "    palette (str): The color palette to use for the plot. Options: \"deep\", \"muted\", \"pastel\", \"bright\", \"dark\", \"colorblind\", \"viridis\", \"inferno\", \"plasma\", \"magma\", \"cividis\"\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    # Set a Seaborn style=\"white\", \"dark\", \"whitegrid\", \"darkgrid\", \"ticks\"\n",
        "    sns.set(style=\"darkgrid\", font_scale=0.7)\n",
        "\n",
        "    # Define subplot grid\n",
        "    fig, axs = plt.subplots(nrows=int(len(cols_to_plot) / 2), ncols=2, figsize=(15, 12))\n",
        "\n",
        "    # Set figure title\n",
        "    fig.suptitle(\"Examining data distribution\", fontsize=18, y=1)\n",
        "\n",
        "    # Loop through cols_to_plot\n",
        "    for col, ax in zip(cols_to_plot, axs.ravel()):\n",
        "        # Getting an idea of the distribution by counting values\n",
        "        counts = df[col].value_counts().reset_index()\n",
        "        \n",
        "        # Use Seaborn barplot with custom colors\n",
        "        plot = sns.barplot(data=counts, x='index', y=col, ax=ax, palette='viridis')\n",
        "        plot.set_xticklabels(plot.get_xticklabels(), rotation=45)\n",
        "\n",
        "        # Chart formatting\n",
        "        # Chart title\n",
        "        ax.set_title(col)\n",
        "        # Chart y label\n",
        "        ax.set_ylabel(\"count\", fontsize = 10)\n",
        "        # Chart x label\n",
        "        ax.set_xlabel(\"value\", fontsize = 10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4XGexEMsELO"
      },
      "source": [
        "Examining our doc string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzAK6lgVqXGE"
      },
      "outputs": [],
      "source": [
        "help(plot_distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVK1ImPxs3b4"
      },
      "source": [
        "Importantly, this will provide in-context help for any function importanted from a package. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8d__9pUtGaK"
      },
      "outputs": [],
      "source": [
        "help(pd.Categorical)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v12W-dXztknj"
      },
      "source": [
        "Now running our function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fqxf3GqfYzei"
      },
      "outputs": [],
      "source": [
        "# Executing our function\n",
        "plot_distribution(df, cols_to_plot)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N70MHw2HCQGG"
      },
      "source": [
        "> * â„¹ï¸ *Turn your `pandas` dataframe into a Tableau-style User Interface for visual exploration with free and open-source [`pygwalker`](https://github.com/Kanaries/pygwalker).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsLqWeDWVMcO"
      },
      "source": [
        "# **4. A taste of cutting edge AI**  ðŸ¦¾ðŸ¤–\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hN7X2FMuiJ3"
      },
      "source": [
        "#### 4.0 **Section learning objectives** \n",
        ">This section tries to bring some basic NLP (natural language processing) processing from `huggingface` into our workflow. \n",
        "* Brief introduction to `huggingface`\n",
        "* Live demo of whisper\n",
        "* Sentiment analysis of free-text\n",
        "\n",
        "Within AI, in the last few years, language models have improved dramatically. This is due to a sequence based model called the [Transformer](https://www.youtube.com/watch?v=SZorAJ4I-sA). If you have heard of *Chat GPT*, the \"T\" in \"GPT\" is for Transformer!\n",
        "\n",
        "[`huggingface`](https://huggingface.co/) ðŸ¤—, rose to populatity from their `transformers` package. However, since their inception they have built a cutting-edge community, with lots of open-source models that are shared and freely available. [Huggingface spaces](https://huggingface.co/?recent=update-space) allows one to explore and run these models in a browser. Importantly, there are lots of [educational materials](https://huggingface.co/course/) that one can use to upskill.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXwdC05hBLcR"
      },
      "outputs": [],
      "source": [
        "# bash script (!)\n",
        "# installing packages\n",
        "%pip install torch torchaudio transformers ipywebrtc librosa ipywidgets -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEW0u9-1Yvcm"
      },
      "source": [
        "We will use Open AI's [Whisper](https://huggingface.co/openai/whisper-base.en), a high capable automatic speech recongition model.\n",
        "\n",
        "We will use a huggingface [`pipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines), an easy way to perform inference (which may be thought of as predicition)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUOrfaKLBQKP"
      },
      "outputs": [],
      "source": [
        "# importing necessary packages\n",
        "import librosa # for audio processing\n",
        "import torch # for AI\n",
        "import torchaudio # for AI audio processing\n",
        "from transformers import WhisperTokenizer, pipeline # for tokenizing inputs\n",
        "from ipywebrtc import AudioRecorder, CameraStream # for recording audio\n",
        "\n",
        "from IPython.display import Audio # for playing audio\n",
        "\n",
        "# as mentioned, modern AI requires a Graphic Processing Unit (GPU)\n",
        "# we set the device we will run on based on whether a GPU is available\n",
        "# cuda is a software layer, allowing us to control the GPU\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "pipe = pipeline(\n",
        "  \"automatic-speech-recognition\",\n",
        "  model=\"openai/whisper-base.en\",\n",
        "  chunk_length_s=1,\n",
        "  device=device,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0EkcKA8B5wL"
      },
      "outputs": [],
      "source": [
        "camera = CameraStream(constraints={'audio': True,'video':False})\n",
        "recorder = AudioRecorder(stream=camera)\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "recorder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WxKFX11C4HM"
      },
      "outputs": [],
      "source": [
        "# record phrase\n",
        "with open('recording.webm', 'wb') as f:\n",
        "    f.write(recorder.audio.value)\n",
        "!ffmpeg -i recording.webm -ac 1 -f wav recording.wav -y -hide_banner -loglevel panic\n",
        "sig, sr = torchaudio.load(\"recording.wav\")\n",
        "print(sig.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeFdgpNWBUec"
      },
      "outputs": [],
      "source": [
        "#load audio file we just recorded\n",
        "pipe('recording.wav')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq1LjrwBCQGH"
      },
      "source": [
        "Now let's say we wanted to perform a task other that \"automatic-speech-recognition\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQtQ4M3KCQGH"
      },
      "outputs": [],
      "source": [
        "pipe_sentiment = pipeline(\"text-classification\")\n",
        "pipe_sentiment([\"This restaurant is awesome\", \"This restaurant is awful\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4pD6CL2CQGH"
      },
      "source": [
        "What about GPT, can we access that?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KBDaAKfCQGH"
      },
      "outputs": [],
      "source": [
        "generator = pipeline('text-generation', model='openai-gpt')\n",
        "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M2NFPU7CQGH"
      },
      "source": [
        "It's litearlly *that* easy to intergrate high quality opensource models into your workflows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrMqkzY-Kb1N"
      },
      "source": [
        "# **5. Resources**  ðŸ““\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xLdM7OeHSC-Z"
      },
      "source": [
        "\n",
        "## Additonal Resources\n",
        "\n",
        "* [VSCode](https://code.visualstudio.com/download) is a popular Interactive Development Environment (IDE). It makes working with code **much** easier. In addition, their [training material](https://code.visualstudio.com/docs/introvideos/basics) is a great place to learn practical development skills.\n",
        "* [GitHub Copilot](https://www.youtube.com/watch?v=Fi3AJZZregI) is an AI programming assistant. It is effectively auto-correct for code and is based on a transformer model. It is fantastic and can even [answer questions](https://www.youtube.com/watch?v=Fi3AJZZregI).\n",
        "* [Kaggle](https://www.kaggle.com/) is online community of data scientists and machine learning practitioners. They provide free access to GPUs and their forums are particularly good for finding solutions to data science code related problems.\n",
        "* [Swiss Association of Actuaries Data Science initiative](https://www.actuarialdatascience.org/), seems to be the most mature data science effort by any acturaial society.\n",
        "* Prof Marcos Lopez de Prado lectures on [Advances in Financial Machine Learning](https://quantresearch.org/Lectures.htm); academic materials for Cornell University's ORIE 5256 course.\n",
        "* [A curated list of free and open source actuarial software](https://github.com/genedan/actuarial-foss).\n",
        "* [fast.ai](https://www.fast.ai/), a course and python `package` for accessible deep learning.\n",
        "* [Distil](https://distill.pub/), interactive articles about machine learning & AI.\n",
        "* [How to survive the AI revolution](https://www.youtube.com/watch?v=oak1CqqIzug): Stanford Graduate School of Business professors argue that instead of viewing AI as a competitor, we should be embracing it as a **collaborator**.\n",
        "\n",
        "![Alt Text](https://media.tenor.com/oFO9mCbbj98AAAAC/rocket-lift-off.gif)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
