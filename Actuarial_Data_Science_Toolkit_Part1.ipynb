{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "Wc3LVh8qXBCK",
        "L7aj5BRfZg5P",
        "XrMqkzY-Kb1N"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "> [Welcome to Colab](https://colab.research.google.com/), *a hosted `python` notebook environment, where code & documentation live side-by-side.*\n",
        ">\n",
        "> **Note:** if your notebook sections are expanded, please go to *`View > Collapse sections`* \n",
        "---\n",
        "<img src=\"https://www.actuarialsociety.org.za/wp-content/uploads/2022/10/ASSA-LOGO-20224.png\"  width=\"420\" height=\"80\">\n",
        "\n",
        "# **The Actuarial Data Science Toolkit: A Practical Guide** \n"
      ],
      "metadata": {
        "id": "Wc3LVh8qXBCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **0. Table of contents** \n",
        "---"
      ],
      "metadata": {
        "id": "SryZKhcAQKve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "â†–ï¸ `click task bar`"
      ],
      "metadata": {
        "id": "vVIrkzpfQVgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Introduction**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "KNuDdEzFL0Td"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In recent years the rise of data intensive methods has seen terms like *Big Data*, *Data Science* and *Arficial Intelligence* (AI) have entered public lexicon. The use of such methods in data-rich, mostly internet-based businesses has resulted in tools that may be applied more broadly, and indeed data science tools and methods are used in broad domains. Actuaries have deep expertise with data, yet we have been slow to adapt to use newer data science type tools. \n",
        "\n",
        "> â„¹ï¸ *An actuarial view of Data Science may be found [here](https://actuaries.org.uk/learn/lifelong-learning/data-science-an-actuarial-viewpoint/) and an overview of AI in Actuarial Science may be found [here](https://www.actuarialsociety.org.za/wp-content/uploads/2018/10/2018-Richman-FIN.pdf).*\n",
        "\n",
        "This practical tutorial is designed to *introduce* data science tools that may prove useful in actuarial workflows. No prior knowledge is assumed and concepts are outlined with provided **reusable code**. Links to more thorough learning materials for individual topics are also provided. The hope is that it will spark curiousity in members and that they will use such tools more regularly in their day-to-day work. \n",
        "\n",
        "> â„¹ï¸ *Typical actuarial work needs better data maturity, along with practical upskilling to take full advantage of advanced AI. As a profession of thought leaders we should help shape our industries' futures to take advantage of these rapidly evolving technologies.*\n",
        " "
      ],
      "metadata": {
        "id": "F7A1rKE8QhNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. From Excel to `python`**  ðŸ\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "WtYRhAkvQSXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **2.0 Section learning objectives** \n",
        "\n",
        ">The purpose of this section is to introduce a member, who is proficient in Excel, to basic `python` tools. This is intended for an absolute beginner and  afterwards the member should feel comfortable using `python` for common tasks they would normally perform in Excel. \n",
        "  * Introduce notebooks\n",
        "  * `wget` data \n",
        "  * `pip install`\n",
        "  * `import` packages\n",
        "  * Link to `pd` learning resoruces\n",
        "  * Note on `pd==2.0.0` \n",
        "  * `pd` feature: summarise data\n",
        "  * `pd` feature: read data into df\n",
        "  * `pd` feature: rename columns\n",
        "  * `pd` feature: dropna\n",
        "  * `pd` feature: drop duplicates `TODO`\n",
        "  * `pd` feature: query\n",
        "  * `pd` feature: filter \n",
        "  * `pd` feauture: conditional formatting"
      ],
      "metadata": {
        "id": "L7aj5BRfZg5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1 **We can't ignore Excel, for now...**\n",
        "\n",
        "There is no denying that the majority of actuarial work is done in Excel (for now!). As such, learning to intergrate Excel as a tool in your data science workflow is essential for practical benefit. \n",
        "\n",
        "\n",
        "#### 2.2 **Why `python`?**\n",
        "\n",
        "In the Data Science community there has been fierce debate over whether `python` or `R` was the better data science coding language. While both languages are capible, we have opted for `python` as it has become the defacto language used for modern AI tools. It is also worth mentioning that `julia`, a newer language is growing and there is already an [actuarial `julia` community](https://juliaactuary.org/).\n",
        "\n",
        "A major advantage of Python is the popularity of notebooks, like this document. A notebook is a \"living document\" with live code and documentation displayed side-by-side. We have opted to use Colab as it provides free GPU access, needed for modern AI.  \n",
        "\n",
        "#### 2.3 **Getting data**\n",
        "Firstly, download data from the Actuarial Society's GitHub, a platform used to store code."
      ],
      "metadata": {
        "id": "O3KFkU08M-Vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone the repository actuarialsociety's github \n",
        "# bash script (!)\n",
        "!git clone https://github.com/actuarialsociety/dstoolkit.git"
      ],
      "metadata": {
        "id": "YI6knoNo-0lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4 **How to use `python` to work with Excel**\n",
        "\n",
        "In order to extend the functionality of basic `python` we need to import packages into our notebook environment, into which all code is executed. The `python` pacakges `numpy` and `pandas` are literally two of the most used packages across any language. `numpy` provides broad mathematical functionality and `pandas` allows for data manipulation from multiple sources, including spreadsheets like Excel. \n",
        "\n",
        "Typically packages are installed into our `python` environment using a `pip` command, however as we see below, due to their popularity, these packages are already installed in the Colab notebook environment. "
      ],
      "metadata": {
        "id": "ueR7pJHdEZh0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwHUlMhRV6GG"
      },
      "outputs": [],
      "source": [
        "# bash script (!)\n",
        "!pip install numpy pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> â„¹ï¸ `pandas` has very good documentation and a short introduction can be found [here](https://pandas.pydata.org/docs/user_guide/10min.html).\n",
        "\n",
        "`pandas` a very  capable I/O API (Input/Output capabilities) and can read data from a [variety of sources](https://pandas.pydata.org/docs/user_guide/io.html), including common cloud platforms.\n",
        "\n",
        "In the code-snippet below, we use data from an external `.csv` to append new information to an existing `.xlsx` spreadsheet.\n",
        "\n",
        "> â„¹ï¸ *`pandas` version 2 (`pandas==2.0`) was recently released. You can find the release notes [here](https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html#), as well as a short video on what is new [here](https://www.youtube.com/watch?v=cSLPyRI_ZD8).*"
      ],
      "metadata": {
        "id": "IzuLZRulv59l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# path to data we just downloaded\n",
        "DATA_XLS = \"/content/dstoolkit/model_point.xlsx\"\n",
        "\n",
        "# ingesting our data into a dataframe (df)\n",
        "df = pd.read_excel(DATA_XLS)\n",
        "\n",
        "\n",
        "# now printing out some information about our data\n",
        "print(f\"\\n Getting some basic info about our data... \\n\")\n",
        "print(f\"* size: {df.size} \\n\")\n",
        "print(f\"* shape:{df.shape} \\n\")\n",
        "\n",
        "# now disaplying some information\n",
        "# `display` is like `print` but renders pd df's better \n",
        "display(df.info())\n",
        "\n",
        "print(\"\\n Getting an idea about the ranges of each column... \\n\")\n",
        "display(df.describe())\n",
        "\n",
        "display(df.head(3))"
      ],
      "metadata": {
        "id": "AVTqVAkqR_xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can already see from our `model_point.info()` command that there are `null` values, that we probably want to drop. In reality it probably worth checking if the data could be better sourced to have fewer `null` values."
      ],
      "metadata": {
        "id": "Xsuk9RS5Rd9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n We examine the number of null values in our data before dropping them... \\n\")\n",
        "print(f\"\\n Number of null items: \\n {df.isnull( ).sum()}\")\n",
        "print(f\"\\n Shape before dropping nulls: \\n {df.shape}\")\n",
        "df = df.dropna()\n",
        "print(f\"\\n Shape after dropping nulls: \\n {df.shape}\")\n",
        "\n",
        "print(f\"\\n We probably also what to trim spaces in our column names,convert to lower case and replace spaces with _ \\n\")\n",
        "new_columns = [str(col).strip().lower().replace(\" \",\"_\") for col in list(df.columns)]\n",
        "print(new_columns)\n",
        "\n",
        "# creating dict to rename df\n",
        "rename_dict = dict()\n",
        "for i in range(0,len(list(df.columns))):\n",
        "  rename_dict.update({list(df.columns)[i]:new_columns[i]})\n",
        "\n",
        "# renaming df\n",
        "df = df.rename(columns=rename_dict)\n",
        "\n",
        "# coverting df.sex to a Categorical variable\n",
        "df.sex = pd.Categorical(df.sex)\n",
        "\n",
        "# coverting df.issue to a datetime\n",
        "df.issue_date =  pd.to_datetime(df.issue_date, format='%d/%m/%Y')\n",
        "\n"
      ],
      "metadata": {
        "id": "x3nPSgqZUpsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importantly this allows one to access columns easier when running queries\n",
        "# Setting query wildcards\n",
        "query_age = 55\n",
        "query_sa = 100000\n",
        "\n",
        "# Querying with wildcards\n",
        "(df\n",
        " .query('age_at_entry > @query_age and sum_assured > @query_sa')\\\n",
        " .head(3))"
      ],
      "metadata": {
        "id": "wBicWNv83ffq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We may wish to group by certrain columns and perform aggregations\n",
        "(df\n",
        ".groupby(['sex','policy_term'])\\\n",
        ".sum_assured\\\n",
        ".agg(['sum', 'mean','count']))"
      ],
      "metadata": {
        "id": "jJU_h_4YAZHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We may also wish to perform conditional formatting on our dataframe\n",
        "\n",
        "(df.\n",
        " sort_values('age_at_entry').head(5)[['age_at_entry', 'premium']]\\\n",
        ".style\\\n",
        ".background_gradient(\"Blues\"))"
      ],
      "metadata": {
        "id": "QdD8RQm-CZMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Make it visual ðŸ‘€** \n",
        "---\n"
      ],
      "metadata": {
        "id": "FnnTMc0luTig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "####**3.0 Section learning objectives** \n",
        "\n",
        ">The purpose of this section is to build upon the previous section; having ingested data, simple visualisation techniques are explored\n",
        "  * Basic plotting by looping with `matplotlib.pyplot`\n",
        "  * Reduce distinct x-axis values for better visualisation\n",
        "  * Write & use a basic python function for repeated code\n",
        "  * Docstrings from own function and from packages\n",
        "  \n",
        "After looking at all available columns, we decide we wish to drop `'model_point'` & `'uuid'`, and thus only focus on columns with index 2 and above."
      ],
      "metadata": {
        "id": "vct36LBila4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#only focusing on columns with index 2 and above.\n",
        "cols_to_plot = list(df.columns)[2:]\n",
        "cols_to_plot"
      ],
      "metadata": {
        "id": "XwnMYitKfDa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define subplot grid\n",
        "fig, axs = plt.subplots(nrows=int(len(cols_to_plot)/2) + 1, ncols=2, figsize=(15, 12))\n",
        "\n",
        "# setting figure title\n",
        "fig.suptitle(\"Examining df... take 1\", fontsize=18, y=0.95)\n",
        "\n",
        "# loop through cols_to_plot\n",
        "for col, ax in zip(cols_to_plot, axs.ravel()):\n",
        "    # getting an idea of the distribution by counting values\n",
        "    df[col].value_counts().plot(kind='bar',ax=ax)\n",
        "\n",
        "    # chart formatting\n",
        "    # chart title\n",
        "    ax.set_title(col)\n",
        "    # chart y lable label \n",
        "    ax.set_ylabel(\"count\")\n",
        "    # chart x lable label \n",
        "    ax.set_xlabel(\"value\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DZLKhjuxvut0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that for `issue_date, sum_assured, premium` there are too many values we are counting by, and thus we struggle to see what is happening. \n",
        "\n",
        "We will create new variables that aggregate these columns for fewer values and replace those columns in our `cols_to_plot` list. \n"
      ],
      "metadata": {
        "id": "TXHmCR8SQysu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['issue_date_month'] = df['issue_date'].dt.to_period('M')\n",
        "\n",
        "print(f\"For `issue_date_month`, we previously had {df['issue_date'].nunique() } unique values; now we have {df['issue_date_month'].nunique() } \")"
      ],
      "metadata": {
        "id": "Xkzy0vVDS6Kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using list comprehension to creat bins\n",
        "bins = [(df['sum_assured'].max())*(i/10) for i in range(1,11)]\n",
        "df['sum_assured_binned'] = pd.cut(df['sum_assured'], bins)\n",
        "print(f\"For `sum_assured`, we previously had {df['sum_assured'].nunique() } unique values; now we have {df['sum_assured_binned'].nunique() } \")\n"
      ],
      "metadata": {
        "id": "WnTHYhATg88S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using list comprehension to creat bins\n",
        "bins = [(df['premium'].max())*(i/10) for i in range(1,11)]\n",
        "df['premium_binned'] = pd.cut(df['premium'], bins)\n",
        "print(f\"For `premium`, we previously had {df['premium'].nunique() } unique values; now we have {df['premium_binned'].nunique() } \")"
      ],
      "metadata": {
        "id": "jkNjyieDkj1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rename_dict = dict()\n",
        "for i in cols_to_plot:\n",
        "  rename_dict.update({i:i})\n",
        "rename_dict[\"issue_month\"] = \"issue_month_month\"\n",
        "rename_dict[\"sum_assured\"] = \"sum_assured_binned\"\n",
        "rename_dict[\"premium\"] = \"premium_binned\"\n",
        "\n",
        "cols_to_plot = [*map(rename_dict.get, cols_to_plot)]\n",
        "cols_to_plot"
      ],
      "metadata": {
        "id": "ooD957bDmVba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have updated `cols_to_plot` we will turn our plot commands into a function. Note the description (known as a Doc String) which outlines the detail of inputs and outputs. This will appear if you say `help(function_name)`\n",
        "\n",
        "Notebooks are messy and often used for ad-hoc investigations. In a production environment we would want to turn all our commands into neat functions. "
      ],
      "metadata": {
        "id": "imRUnmP8ohZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: This must be blank in the tutorial after assert\n",
        "def plot_data_cols(data, plot_cols):\n",
        "    \"\"\"\n",
        "    Returns plot of plot_cols from data\n",
        "    Args:\n",
        "      * data: dataframe with columns names in plot_cols\n",
        "      * plot_cols: column names in data that will be plott\n",
        "    Returns:\n",
        "      * plot of plot_cols from data\n",
        "    \"\"\"\n",
        "    # assert statements check our inputs are correct\n",
        "    assert ([col in list(data.columns) for col in plot_cols] == [True]*len(plot_cols))\n",
        "  \n",
        "    # define subplot grid\n",
        "    fig, axs = plt.subplots(nrows=int(len(plot_cols)/2) + 1, ncols=2, figsize=(15, 12))\n",
        "\n",
        "    # setting figure title\n",
        "    fig.suptitle(\"Examining df... with our function\", fontsize=18, y=0.95)\n",
        "\n",
        "    # loop through cols_to_plot\n",
        "    for col, ax in zip(plot_cols, axs.ravel()):\n",
        "        # getting an idea of the distribution by counting values\n",
        "        data[col].value_counts().plot(kind='bar',ax=ax)\n",
        "\n",
        "        # chart formatting\n",
        "        # chart title\n",
        "        ax.set_title(col)\n",
        "        # chart y lable label \n",
        "        ax.set_ylabel(\"count\")\n",
        "        # chart x lable label \n",
        "        ax.set_xlabel(\"value\")\n",
        "\n",
        "    plt.show()\n",
        "    # loop through cols_to_plot\n",
        "    for col, ax in zip(plot_cols, axs.ravel()):\n",
        "        # getting an idea of the distribution by counting values\n",
        "        df[col].value_counts().plot(kind='bar',ax=ax)\n",
        "\n",
        "        # chart formatting\n",
        "        # chart title\n",
        "        ax.set_title(col)\n",
        "        # chart y lable label \n",
        "        ax.set_ylabel(\"count\")\n",
        "        # chart x lable label \n",
        "        ax.set_xlabel(\"value\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "vNmb4VfHpIg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examining our doc string"
      ],
      "metadata": {
        "id": "Q4XGexEMsELO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(plot_data_cols)"
      ],
      "metadata": {
        "id": "JzAK6lgVqXGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importantly, this will provide in-context help for any function importanted from a package. "
      ],
      "metadata": {
        "id": "VVK1ImPxs3b4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(pd.Categorical)"
      ],
      "metadata": {
        "id": "e8d__9pUtGaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now running our function"
      ],
      "metadata": {
        "id": "v12W-dXztknj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data_cols(df, cols_to_plot)"
      ],
      "metadata": {
        "id": "Fqxf3GqfYzei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. A taste of cutting edge AI**  ðŸ¦¾ðŸ¤–\n",
        "---\n"
      ],
      "metadata": {
        "id": "VsLqWeDWVMcO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.0 **Section learning objectives** \n",
        ">This section tries to bring some basic NLP (natural language processing) processing from `huggingface` into our workflow. \n",
        "* Brief introduction to `huggingface`\n",
        "* Live demo of whisper\n",
        "* Sentiment analysis of free-text\n",
        "* `pd` feature: merge \n",
        "* `pd` feature: pivot \n",
        "* `pd` feature: Write Excel output\n",
        "\n",
        "Within AI, within the last few years, language models have improved dramatically. This is due to a sequence based model called the [Transformer](https://www.youtube.com/watch?v=SZorAJ4I-sA), which is now applied beyond language. If you have heard of *Chat GPT*, the \"T\" in \"GPT\" is for \"Transformer\"!\n",
        "\n",
        "[`huggingface`](https://huggingface.co/), rose to populatity from their `transformers` package. However, since their inception they have built a cutting-edge community, with lots of open-source models that are shared and freely available. [Huggingface spaces](https://huggingface.co/?recent=update-space) allows one to explore and run these model in a browser. Importantly, there are lots of [educational materials](https://huggingface.co/course/) that one can use to upskill.\n"
      ],
      "metadata": {
        "id": "0hN7X2FMuiJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bash script (!)\n",
        "! pip install -q transformers ipywebrtc"
      ],
      "metadata": {
        "id": "TXwdC05hBLcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing necessary packages\n",
        "import librosa\n",
        "import torch\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
        "from ipywebrtc import AudioRecorder, CameraStream\n",
        "import torchaudio\n",
        "from IPython.display import Audio\n",
        "\n",
        "#load pre-trained model and tokenizer from huggingface hub\n",
        "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\") #TODO replace with OpenAI Whisper\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\") #TODO replace with OpenAI Whisper"
      ],
      "metadata": {
        "id": "WUOrfaKLBQKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "camera = CameraStream(constraints={'audio': True,'video':False})\n",
        "recorder = AudioRecorder(stream=camera)\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "recorder"
      ],
      "metadata": {
        "id": "d0EkcKA8B5wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# record phrase\n",
        "with open('recording.webm', 'wb') as f:\n",
        "    f.write(recorder.audio.value)\n",
        "!ffmpeg -i recording.webm -ac 1 -f wav recording.wav -y -hide_banner -loglevel panic\n",
        "sig, sr = torchaudio.load(\"recording.wav\")\n",
        "print(sig.shape)\n",
        "Audio(data=sig, rate=sr)"
      ],
      "metadata": {
        "id": "7WxKFX11C4HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load audio file we just recorded\n",
        "speech, rate = librosa.load(\"recording.wav\",sr=16000)"
      ],
      "metadata": {
        "id": "UeFdgpNWBUec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizing input data\n",
        "input_values = tokenizer(speech, return_tensors = 'pt').input_values\n",
        "\n",
        "#store logits (non-normalized predictions)\n",
        "logits = model(input_values).logits\n",
        "\n",
        "#store predicted id's\n",
        "predicted_ids = torch.argmax(logits, dim =-1)\n",
        "\n",
        "#decode the audio to generate text\n",
        "transcription = tokenizer.decode(predicted_ids[0])\n",
        "print(transcription)"
      ],
      "metadata": {
        "id": "r8MBIdTxBYzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`#TODO`\n",
        "* create second data source of *Underwriter text comments*, where only some `uuid` values match `model_point` so that `merge` is needed to extract relevant items called `underwrite.csv`\n",
        "* Perform Sentiment analysis with appopriate `huggingface` model\n",
        "* Give discount to policies with positive underwriter sentiment"
      ],
      "metadata": {
        "id": "duteC7l3ImX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Resources**  ðŸ““\n",
        "---"
      ],
      "metadata": {
        "id": "XrMqkzY-Kb1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* VSCode is a popular Interactive Development Environment. It makes working with code easier. In addition their [training material](https://code.visualstudio.com/docs/introvideos/basics) is great for place to learn practical development skills.\n",
        "\n",
        "* [Kaggle](https://www.kaggle.com/) is online community of data scientists and machine learning practitioners. They provide free access to GPUs for deep learning and their forums are particularly good for finding solutions to data science code related problems.\n",
        "\n",
        "* [Swiss Association of Actuaries Data Science initiative](https://www.actuarialdatascience.org/), seems to be the most mature data science effort by any acturaial society.\n",
        "\n",
        "* [fast.ai](https://www.fast.ai/), a course and python `package` for accessible deep learning.\n",
        "\n",
        "* [Distil](https://distill.pub/), interactive articles about machine learning."
      ],
      "metadata": {
        "id": "xLdM7OeHSC-Z"
      }
    }
  ]
}